## Describe components of an ML pipeline (for example, data collection, exploratory data analysis [EDA], data pre-processing, feature engineering, model training, hyperparameter tuning, evaluation, deployment, monitoring).
**Data Collection**: Gathering relevant data from various sources such as databases, APIs, or sensors.

**Exploratory Data Analysis (EDA)**: Understanding the data by visualizing distributions, identifying trends, and detecting outliers.

**Data Pre-Processing**: Cleaning and transforming data to ensure quality, such as handling missing values and normalizing data.

**Feature Engineering**: Selecting, creating, or transforming input features to improve model performance.

**Model Training**: Using labeled data to train the chosen algorithm to recognize patterns and make predictions.

**Hyperparameter Tuning**: Optimizing parameters like learning rate and batch size to improve model performance.

**Evaluation**: Testing the model using metrics like accuracy, precision, recall, or F1 score to assess its effectiveness.

**Deployment**: Integrating the trained model into a production environment for real-world use.

**Monitoring**: Continuously tracking model performance and data drift to ensure consistent results.

## Understand sources of ML models (for example, open source pre-trained models, training custom models).
**Open Source Pre-Trained Models**: Pre-built models available from libraries (e.g., Hugging Face, TensorFlow Hub) that save time and resources by skipping initial training.

**Training Custom Models**: Building models from scratch or fine-tuning pre-trained models for specific use cases using proprietary datasets.

## Describe methods to use a model in production (for example, managed API service, self-hosted API).
**Managed API Service**: Hosting the model using cloud services like Amazon SageMaker Endpoints to simplify scaling and management.

**Self-Hosted API**: Deploying the model on custom infrastructure or on-premises servers for greater control.

## Identify relevant AWS services and features for each stage of an ML pipeline (for example, SageMaker, Amazon SageMaker Data Wrangler, Amazon SageMaker Feature Store, Amazon SageMaker Model Monitor).
**Data Collection**: AWS Glue, Amazon Kinesis, Amazon S3.

**EDA**: Amazon SageMaker Studio, AWS Glue DataBrew.

**Data Pre-Processing**: Amazon SageMaker Data Wrangler, AWS Glue.

**Feature Engineering**: Amazon SageMaker Feature Store.

**Model Training**: Amazon SageMaker Training Jobs, AWS Deep Learning AMIs.

**Hyperparameter Tuning**: Amazon SageMaker Automatic Model Tuning.

**Evaluation**: Amazon SageMaker Debugger.

**Deployment**: Amazon SageMaker Endpoints, Amazon Elastic Kubernetes Service (EKS).

**Monitoring**: Amazon SageMaker Model Monitor.

## Understand fundamental concepts of ML operations (MLOps) (for example, experimentation, repeatable processes, scalable systems, managing technical debt, achieving production readiness, model monitoring, model re-training).
**Experimentation**: Testing different models and configurations to optimize performance.

**Repeatable Processes**: Automating workflows using tools like CI/CD pipelines to ensure consistency.

**Scalable Systems**: Designing infrastructure to handle increasing workloads and data.

**Managing Technical Debt**: Maintaining code quality and minimizing legacy system issues.

**Achieving Production Readiness**: Ensuring the model is reliable, secure, and optimized for deployment.

**Model Monitoring**: Tracking performance metrics and identifying data or concept drift.

**Model Re-Training**: Regularly updating models with new data to maintain accuracy.

## Understand model performance metrics (for example, accuracy, Area Under the ROC Curve [AUC], F1 score) and business metrics (for example, cost per user, development costs, customer feedback, return on investment [ROI]) to evaluate ML models.
**Performance Metrics**:
- **Accuracy**: Percentage of correct predictions.
- **Area Under the ROC Curve (AUC)**: Measures the trade-off between true positive rate and false positive rate.
- **F1 Score**: Harmonic mean of precision and recall, balancing false positives and false negatives.

**Business Metrics**:
- **Cost per User**: Operational cost divided by the number of users served.
- **Development Costs**: Expenses incurred during model development and deployment.
- **Customer Feedback**: Insights from users about the model's effectiveness.
- **Return on Investment (ROI)**: Quantifies the business value generated by the model.